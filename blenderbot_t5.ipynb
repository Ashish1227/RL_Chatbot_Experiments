{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVozb6H1kUwY",
        "outputId": "2e1f3f99-403e-410d-b1d2-844f50e4c65b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pfrl@ git+https://github.com/voidful/pfrl.git\n",
            "  Cloning https://github.com/voidful/pfrl.git to /tmp/pip-install-q8ppy5t8/pfrl_202a3af612d54315aba3e8bd3883333c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/voidful/pfrl.git /tmp/pip-install-q8ppy5t8/pfrl_202a3af612d54315aba3e8bd3883333c\n",
            "  Resolved https://github.com/voidful/pfrl.git to commit 2ad3d51a7a971f3fe7f2711f024be11642990d61\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (2.0.0+cu118)\n",
            "Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (3.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textrl==0.2.13 in /usr/local/lib/python3.10/dist-packages (0.2.13)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from textrl==0.2.13) (0.25.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from textrl==0.2.13) (4.29.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->textrl==0.2.13) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->textrl==0.2.13) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->textrl==0.2.13) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->textrl==0.2.13) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->textrl==0.2.13) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->textrl==0.2.13) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->textrl==0.2.13) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->textrl==0.2.13) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->textrl==0.2.13) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->textrl==0.2.13) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
        "!pip install textrl==0.2.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "rXYwDqGBCuDs"
      },
      "outputs": [],
      "source": [
        "from textrl import TextRLEnv,TextRLActor\n",
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead , BlenderbotForConditionalGeneration\n",
        "import logging\n",
        "import sys\n",
        "import pfrl\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iyyhamsC7nY",
        "outputId": "2956f49a-f527-4652-b77b-64904d7bb641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shsUvGB9CxfZ",
        "outputId": "f9be8b65-4fae-42e8-8a94-354f05e0a8f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1352: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "another_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "another_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\").to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "aYubamasOXKB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
        "mname = \"facebook/blenderbot-400M-distill\"\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(mname).to(\"cuda:0\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(mname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "ha-iuU_NDk91"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-90M\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-90M\").to(\"cuda:0\")\n",
        "# model.eval()\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFiZkq_oIQlU",
        "outputId": "2ebcefb1-696a-4158-eca9-abdf07d4bf3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot 1: ['What is your outlook on life?']\n",
            "Bot 0 : [' My outlook is to be happy with who I am and to be content with where I am in life.']\n",
            "Bot 1 : [\" That's a good outlook to have. What do you do for a living, if you don't mind me asking?\"]\n",
            "Bot 0 : [\" I work in a call center. It's not the most glamorous job in the world, but it pays the bills.\"]\n",
            "Bot 1 : [' I used to work in one as well.  It was a lot of fun.  What do you do in your free time?']\n",
            "Bot 0 : [' I like to play video games and hang out with my friends. What about you? Do you have any hobbies?']\n",
            "Bot 1 : [\" Video games are fun, but I don't have a lot of time for them.\"]\n",
            "Bot 0 : [' I know what you mean, they can be very time consuming. What kind of games do you like to play?']\n",
            "Bot 1 : [' I like RPGs and adventure games the most. What about you? Do you have any favorites?']\n",
            "Bot 0 : [\" I love adventure and RPG's as well. I have a lot of favorites.\"]\n",
            "Bot 1 : [' Adventure is a great genre. What is your favorite RPG? I really like Dungeons and Dragons.']\n"
          ]
        }
      ],
      "source": [
        "UTTERANCE = [\"What is your outlook on life?\"]\n",
        "print(\"Bot 1:\",UTTERANCE)\n",
        "for i in range(10):\n",
        "    \n",
        "    inputs = tokenizer(UTTERANCE, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    reply_ids = model.generate(**inputs)\n",
        "   \n",
        "    UTTERANCE = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)\n",
        "    print(\"Bot\",str(i%2),\":\",UTTERANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "o4YI1ikpDo3X"
      },
      "outputs": [],
      "source": [
        "def get_emotion(text):\n",
        "    input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "    output = model.generate(input_ids=input_ids,max_length=200)\n",
        "    dec = [tokenizer.decode(ids) for ids in output]\n",
        "    label = dec[0]\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "zKS5rAKXUi7D"
      },
      "outputs": [],
      "source": [
        "dialogue_list = []\n",
        "\n",
        "with open(\"/content/dialogues_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        dialogue_parts = line.strip().split(\"__eou__\")\n",
        "        dialogue_list.extend(dialogue_parts)\n",
        "\n",
        "observation_list = dialogue_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgpFvG5UYG_4",
        "outputId": "136b67aa-0ea4-435a-f243-df7c07e0fc12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The list does not contain empty strings.\n"
          ]
        }
      ],
      "source": [
        "# Check if the list contains empty strings\n",
        "has_empty_strings = any(element.isspace() for element in observation_list)\n",
        "\n",
        "if has_empty_strings:\n",
        "    print(\"The list contains empty strings.\")\n",
        "else:\n",
        "    print(\"The list does not contain empty strings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "5pXNS7qYFVgt"
      },
      "outputs": [],
      "source": [
        "class MyRLEnv(TextRLEnv):\n",
        "    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
        "        reward = 0\n",
        "        if finish:\n",
        "            predicted_text = another_tokenizer.convert_tokens_to_string(predicted_list)\n",
        "            # sentiment classifier\n",
        "            if(get_emotion(predicted_text)[6:] == 'joy'):\n",
        "                reward = -1000\n",
        "            elif(get_emotion(predicted_text)[6:] == 'love'):\n",
        "                reward = -1000\n",
        "            elif(get_emotion(predicted_text)[6:] == 'surprise'):\n",
        "                reward = -1000\n",
        "            elif(get_emotion(predicted_text)[6:] == 'sadness'):\n",
        "                reward = 100\n",
        "            elif(get_emotion(predicted_text)[6:] == 'fear'):\n",
        "                reward = 100\n",
        "            elif(get_emotion(predicted_text)[6:] == 'anger'):\n",
        "                reward = 100\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "3nd9-oYpk-gU"
      },
      "outputs": [],
      "source": [
        "env = MyRLEnv(model, another_tokenizer, observation_input=observation_list,compare_sample=3)\n",
        "actor = TextRLActor(env,model,tokenizer)\n",
        "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLVxRfWZlBRJ",
        "outputId": "3bf28ad7-22f5-459a-b2bb-9f238e9bd147"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<textrl.actor.TextPPO at 0x7f55ae688910>,\n",
              " [{'average_value': -0.72880656,\n",
              "   'average_entropy': 4.606346,\n",
              "   'average_value_loss': 0.06618846761994064,\n",
              "   'average_policy_loss': 0.013455330338329076,\n",
              "   'n_updates': 334,\n",
              "   'explained_variance': 0.9115965218817765,\n",
              "   'eval_score': 0.0},\n",
              "  {'average_value': -0.42905903,\n",
              "   'average_entropy': 4.448599,\n",
              "   'average_value_loss': 0.03760535953566432,\n",
              "   'average_policy_loss': -0.001881647133268416,\n",
              "   'n_updates': 668,\n",
              "   'explained_variance': 0.2706570932651071,\n",
              "   'eval_score': 0.0},\n",
              "  {'average_value': -0.30766016,\n",
              "   'average_entropy': 4.539911,\n",
              "   'average_value_loss': 0.009971934825880453,\n",
              "   'average_policy_loss': 0.005531531348824501,\n",
              "   'n_updates': 1002,\n",
              "   'explained_variance': 0.413468489106787,\n",
              "   'eval_score': 0.0}])"
            ]
          },
          "execution_count": 268,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pfrl.experiments.train_agent_with_evaluation(\n",
        "    agent,\n",
        "    env,\n",
        "    steps=300,\n",
        "    eval_n_steps=None,\n",
        "    eval_n_episodes=1,       \n",
        "    train_max_episode_len=100,  \n",
        "    eval_interval=10,\n",
        "    outdir='elon_musk_dogecoin', \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTbswcd2CNE7",
        "outputId": "5477c21a-10cd-4301-f5bd-9b456339ddcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot 1: ['What is your outlook on life?']\n",
            "Bot 0 : [' My outlook is to be happy with who I am and to be content with where I am in life.']\n",
            "Bot 1 : [\" That's a good outlook to have. What do you do for a living, if you don't mind me asking?\"]\n",
            "Bot 0 : [\" I work in a call center. It's not the most glamorous job in the world, but it pays the bills.\"]\n",
            "Bot 1 : [' I used to work in one as well. It was not glamourous at all.']\n",
            "Bot 0 : [\" Yeah, it wasn't glamorous, but it was a good way to make some extra money.\"]\n",
            "Bot 1 : [\" That's true, I guess I was just a little envious of the guy who was able to do that.\"]\n",
            "Bot 0 : [' Yeah, I know what you mean.  I wish I could do that too.  ']\n",
            "Bot 1 : [\" I know, right?  It's just so easy to get caught up in the moment.\"]\n",
            "Bot 0 : [' Yes, it is. I wish I could go back in time and do it all over again.']\n",
            "Bot 1 : [\" I know, right?  It was so much fun.  I don't know what I would do without it.\"]\n"
          ]
        }
      ],
      "source": [
        "UTTERANCE = [\"What is your outlook on life?\"]\n",
        "print(\"Bot 1:\",UTTERANCE)\n",
        "for i in range(10):\n",
        "    \n",
        "    inputs = tokenizer(UTTERANCE, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    reply_ids = model.generate(**inputs)\n",
        "   \n",
        "    UTTERANCE = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)\n",
        "    print(\"Bot\",str(i%2),\":\",UTTERANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "kQ4u41GQIHfg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
